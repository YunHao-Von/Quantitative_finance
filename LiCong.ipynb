{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['000503.SZ', '000820.SZ', '300122.SZ', '600233.SH', '002120.SZ',\n",
      "       '002625.SZ', '600161.SH', '002522.SZ', '000025.SZ', '300136.SZ',\n",
      "       '600435.SH', '600764.SH', '600876.SH', '600570.SH', '002468.SZ',\n",
      "       '002346.SZ', '600698.SH', '600610.SH', '300601.SZ', '002797.SZ',\n",
      "       '000813.SZ', '002127.SZ', '002095.SZ', '600679.SH', '000923.SZ',\n",
      "       '000068.SZ', '000697.SZ', '300308.SZ', '300482.SZ', '002044.SZ',\n",
      "       '002451.SZ', '002252.SZ', '002180.SZ', '002569.SZ', '002387.SZ',\n",
      "       '300101.SZ', '603881.SH', '000831.SZ', '601020.SH', '300085.SZ',\n",
      "       '300506.SZ', '000766.SZ', '600760.SH', '603986.SH', '002694.SZ',\n",
      "       '601069.SH', '002622.SZ', '002515.SZ', '300191.SZ', '002530.SZ',\n",
      "       '300142.SZ', '300558.SZ', '603058.SH', '002676.SZ', '300529.SZ',\n",
      "       '600393.SH', '002575.SZ', '600771.SH', '300474.SZ', '600158.SH',\n",
      "       '600779.SH', '603859.SH', '300496.SZ', '300618.SZ', '300302.SZ',\n",
      "       '600436.SH', '000806.SZ', '300628.SZ', '600110.SH', '603718.SH',\n",
      "       '000815.SZ', '600111.SH', '300450.SZ', '300466.SZ', '000839.SZ',\n",
      "       '002192.SZ', '002509.SZ', '002602.SZ', '600715.SH', '002287.SZ',\n",
      "       '601882.SH', '603799.SH', '002230.SZ', '000403.SZ', '000738.SZ',\n",
      "       '000017.SZ', '002460.SZ', '002030.SZ', '002773.SZ', '300368.SZ',\n",
      "       '603888.SH', '600053.SH', '002147.SZ', '603000.SH', '600276.SH',\n",
      "       '002366.SZ', '002213.SZ', '300024.SZ', '002782.SZ', '300276.SZ'],\n",
      "      dtype='object', name='ts_code')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "year=['2017','2018','2019','2020']#设置年份\n",
    "# month=[('0101','0331'),('0401','0630'),('0701','0930'),('1001','1231')]#设置时间段\n",
    "m=['-01','-02','-03','-04','-05','-06','-07','-08','-09','-10','-11','-12']#设置月份\n",
    "def get__score(df_basic_filled_list,d,t):#对单一指标进行排序\n",
    "    df=df_indicator_filled_list\n",
    "    df=df_indicator_filled_list[t[0:4]][t[4:]]\n",
    "    df=df.loc[:,['ts_code',d]]\n",
    "    dg=df.groupby('ts_code')\n",
    "    dg=dg.mean()\n",
    "    dg=dg.sort_values(d,ascending=False)\n",
    "    rank=dg.rank()\n",
    "    return rank\n",
    "def unit_socre(df,ind,t):#联合单一指标获得联合指标排序\n",
    "    tmp=get__score(df,ind[0],t)\n",
    "    rank=tmp\n",
    "    for i in range(1,len(ind)):\n",
    "        tmp=get__score(df,ind[i],t)\n",
    "        rank = pd.merge(rank,tmp,how='inner',on='ts_code')\n",
    "    rank['score']=rank.apply(lambda x:x.sum(),axis=1)\n",
    "    return rank\n",
    "# =============================================================================\n",
    "#step1.读取数据 \n",
    "# =============================================================================\n",
    "#df_indicator=pd.read_csv('D:\\\\金融建模\\\\数据\\\\import_data\\\\fin_indicator.csv')#读入数据\n",
    "#df_indicator=pd.read_csv('D:\\\\金融建模\\\\数据\\\\import_data\\\\table.csv')#读入数据\n",
    "df_indicator=pd.read_csv('daily_basic.csv')#读入数据\n",
    "#df_basic=df_basic.loc[:,['ann_date','total_share','ts_code']]\n",
    "# =============================================================================\n",
    "# step2.数据基本处理\n",
    "# =============================================================================\n",
    "df_indicator_filled=df_indicator.fillna(0)#填充空缺\n",
    "df_indicator_filled['trade_date']=pd.to_datetime(df_indicator_filled['trade_date'],format='%Y%m%d')#设置时间戳\n",
    "df_indicator_filled=df_indicator_filled.set_index('trade_date')#设置index为时间序列\n",
    "df_indicator_filled_list={}#字典保存按年份和月份分割的Dataframe\n",
    "# =============================================================================\n",
    "# step3.划分数据生成字典\n",
    "# =============================================================================\n",
    "for i in year:\n",
    "    month_dict={}\n",
    "    for j in m:\n",
    "        month_dict[j]=df_indicator_filled.loc[i+j,:]\n",
    "    df_indicator_filled_list[i]=month_dict\n",
    "# =============================================================================\n",
    "# 设置指定指标\n",
    "#fina_indicator=['roa','roe','debt_to_assets','bps']#选取的指标\n",
    "#table=['total_share']\n",
    "daily=['pe','pb','ps','total_mv']\n",
    "# =============================================================================\n",
    "score_indicator=unit_socre(df_indicator_filled_list,daily,'2017-04')#获得指定指标序列和指定时间的打分\n",
    "select=score_indicator.sort_values(by='score',ascending=False).head(100)\n",
    "print(select.index)\n",
    "# t='2020-06'\n",
    "# d='roa'\n",
    "# df=df_indicator_filled_list\n",
    "# df=df_indicator_filled_list[t[0:4]][t[4:]]\n",
    "# df=df.loc[:,['ts_code',d]]\n",
    "# dg=df.groupby('ts_code')\n",
    "# dg=dg.mean()\n",
    "# dg=dg.sort_values(d,ascending=False)\n",
    "# rank=dg.rank()\n",
    "# rank=pd.DataFrame(index=)\n",
    "# for i in fina_indicator:\n",
    "#     tmp=get__score(df,i,t)\n",
    "#     rank = pd.merge(rank,tmp,how='inner',on='ts_code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=list(select.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"company_label.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data[\"code\"]=data[\"sec_code\"].apply(lambda x:re.findall(r\"(.+)_.+_.+\",x)[0]+\".\"+re.findall(r\".+_(.+)_.+\",x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1=data[data[\"code\"].isin(l)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-1842ee83424d>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data1[\"time\"]=pd.to_datetime(data1[\"news_time\"])\n"
     ]
    }
   ],
   "source": [
    "data1[\"time\"]=pd.to_datetime(data1[\"news_time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=pd.DataFrame(columns=[\"pos\",\"neg\",\"neu\",\"code\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=pd.DataFrame(columns=[\"pos\",\"neg\",\"neu\",\"公司\",\"code\"])\n",
    "for i in data1[\"code\"].unique():\n",
    "    t1=data1[data1[\"code\"]==i]\n",
    "    t1.index=data1[data1[\"code\"]==i][\"time\"]\n",
    "    t2=t1.resample(\"1d\")[\"pos\",\"neg\",\"neu\"].mean()\n",
    "    t2[\"code\"]=i\n",
    "    t2[\"公司\"]=data1[data1[\"code\"]==i].company_name.unique()[0]\n",
    "    t=pd.concat([t,t2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"event_label.csv\")\n",
    "event=df.merge(data1,on=\"news_id\",how=\"right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in event[\"event_name\"].unique()[1:]:\n",
    "    event[i]=event[\"event_name\"].apply(lambda x :1 if x==i else 0)\n",
    "    event[i]=event[i].astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls=list(event.columns[13:])\n",
    "ls.append(\"公司\")\n",
    "ls.append(\"code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(event.code.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "T=pd.DataFrame(columns=ls)\n",
    "for i in event[\"code\"].unique():\n",
    "    T1=event[event[\"code\"]==i]\n",
    "    T1.index=event[event[\"code\"]==i][\"time\"]\n",
    "    T2=T1.resample(\"1d\")[list(event.columns[13:])].sum()\n",
    "    T2[\"code\"]=i\n",
    "    T2[\"公司\"]=event[event[\"code\"]==i].company_name.unique()[0]\n",
    "    T=pd.concat([T,T2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 pos       neg       neu   公司       code\n",
      "2017-01-01  0.268822  0.183989  0.547200  新华网  603888.SH\n",
      "2017-01-02  0.151290  0.061890  0.786810  新华网  603888.SH\n",
      "2017-01-03  0.219722  0.098174  0.682117  新华网  603888.SH\n",
      "2017-01-04  0.309385  0.117896  0.572712  新华网  603888.SH\n",
      "2017-01-05  0.265543  0.085724  0.648752  新华网  603888.SH\n",
      "...              ...       ...       ...  ...        ...\n",
      "2020-06-19  0.188033  0.026467  0.785500  美利云  000815.SZ\n",
      "2020-06-20  0.000000  0.000000  0.000000  美利云  000815.SZ\n",
      "2020-06-21  0.000000  0.000000  0.000000  美利云  000815.SZ\n",
      "2020-06-22  0.000000  0.000000  0.000000  美利云  000815.SZ\n",
      "2020-06-23  0.036600  0.038700  0.924700  美利云  000815.SZ\n",
      "\n",
      "[126445 rows x 5 columns]\n",
      "           改革 遭监管 整改 商品价格稳定 大盘下跌 A+轮融资 筹划收购股权 收购股权 遭问询 补贴调整  ... 担保制度 增发保荐报告  \\\n",
      "2017-01-01  0   0  0      0    0     0      0    0   0    0  ...    0      0   \n",
      "2017-01-02  0   0  0      0    0     0      0    0   0    0  ...    0      0   \n",
      "2017-01-03  0   0  0      0    0     0      0    0   0    0  ...    0      0   \n",
      "2017-01-04  0   0  0      0    0     0      0    0   0    0  ...    0      0   \n",
      "2017-01-05  0   0  0      0    0     0      0    0   0    0  ...    0      0   \n",
      "...        ..  .. ..    ...  ...   ...    ...  ...  ..  ...  ...  ...    ...   \n",
      "2020-06-20  0   0  0      0    0     0      0    0   0    0  ...    0      0   \n",
      "2020-06-21  0   0  0      0    0     0      0    0   0    0  ...    0      0   \n",
      "2020-06-22  0   0  0      0    0     0      0    0   0    0  ...    0      0   \n",
      "2020-06-23  0   0  0      0    0     0      0    0   0    0  ...    0      0   \n",
      "2020-06-24  0   0  0      0    0     0      0    0   0    0  ...    0      0   \n",
      "\n",
      "           指数修订 业绩承诺中介意见 业绩承诺实现情况 财务管理制度 资助捐赠制度 NaN    公司       code  \n",
      "2017-01-01    0        0        0      0      0   0  中信国安  000839.SZ  \n",
      "2017-01-02    0        0        0      0      0   0  中信国安  000839.SZ  \n",
      "2017-01-03    0        0        0      0      0   0  中信国安  000839.SZ  \n",
      "2017-01-04    0        0        0      0      0   0  中信国安  000839.SZ  \n",
      "2017-01-05    0        0        0      0      0   0  中信国安  000839.SZ  \n",
      "...         ...      ...      ...    ...    ...  ..   ...        ...  \n",
      "2020-06-20    0        0        0      0      0   0  永吉股份  603058.SH  \n",
      "2020-06-21    0        0        0      0      0   0  永吉股份  603058.SH  \n",
      "2020-06-22    0        0        0      0      0   0  永吉股份  603058.SH  \n",
      "2020-06-23    0        0        0      0      0   0  永吉股份  603058.SH  \n",
      "2020-06-24    0        0        0      0      0   0  永吉股份  603058.SH  \n",
      "\n",
      "[126445 rows x 965 columns]\n"
     ]
    }
   ],
   "source": [
    "print(t)\n",
    "print(T)\n",
    "t.to_csv(\"smallt.csv\",index=False)\n",
    "T.to_csv(\"bigT.csv\",index=False)\n",
    "# final=pd.concat([t,T.iloc[:,:-2]],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 90.1 GiB for an array with shape (963, 12558485) and data type object",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mMemoryError\u001B[0m                               Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-21-a95795c42c76>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mfinal\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mpd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mconcat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mt\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mT\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0miloc\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0maxis\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[0mfinal\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto_csv\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"final_2.csv\"\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mindex\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mFalse\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mF:\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001B[0m in \u001B[0;36mconcat\u001B[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001B[0m\n\u001B[0;32m    282\u001B[0m     )\n\u001B[0;32m    283\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 284\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0mop\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_result\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    285\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    286\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mF:\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001B[0m in \u001B[0;36mget_result\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    494\u001B[0m                 \u001B[0mmgrs_indexers\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mobj\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_data\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mindexers\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    495\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 496\u001B[1;33m             new_data = concatenate_block_managers(\n\u001B[0m\u001B[0;32m    497\u001B[0m                 \u001B[0mmgrs_indexers\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnew_axes\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mconcat_axis\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0maxis\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcopy\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcopy\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    498\u001B[0m             )\n",
      "\u001B[1;32mF:\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001B[0m in \u001B[0;36mconcatenate_block_managers\u001B[1;34m(mgrs_indexers, axes, concat_axis, copy)\u001B[0m\n\u001B[0;32m   2020\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2021\u001B[0m             b = make_block(\n\u001B[1;32m-> 2022\u001B[1;33m                 \u001B[0mconcatenate_join_units\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mjoin_units\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mconcat_axis\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcopy\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mcopy\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2023\u001B[0m                 \u001B[0mplacement\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mplacement\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2024\u001B[0m             )\n",
      "\u001B[1;32mF:\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\concat.py\u001B[0m in \u001B[0;36mconcatenate_join_units\u001B[1;34m(join_units, concat_axis, copy)\u001B[0m\n\u001B[0;32m    244\u001B[0m     \u001B[0mempty_dtype\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mupcasted_na\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_get_empty_dtype_and_na\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mjoin_units\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    245\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 246\u001B[1;33m     to_concat = [\n\u001B[0m\u001B[0;32m    247\u001B[0m         \u001B[0mju\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_reindexed_values\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mempty_dtype\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mempty_dtype\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mupcasted_na\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mupcasted_na\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    248\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mju\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mjoin_units\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mF:\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\concat.py\u001B[0m in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    245\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    246\u001B[0m     to_concat = [\n\u001B[1;32m--> 247\u001B[1;33m         \u001B[0mju\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_reindexed_values\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mempty_dtype\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mempty_dtype\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mupcasted_na\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mupcasted_na\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    248\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mju\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mjoin_units\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    249\u001B[0m     ]\n",
      "\u001B[1;32mF:\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\concat.py\u001B[0m in \u001B[0;36mget_reindexed_values\u001B[1;34m(self, empty_dtype, upcasted_na)\u001B[0m\n\u001B[0;32m    229\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    230\u001B[0m             \u001B[1;32mfor\u001B[0m \u001B[0max\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mindexer\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mindexers\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mitems\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 231\u001B[1;33m                 \u001B[0mvalues\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0malgos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtake_nd\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mvalues\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mindexer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0max\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfill_value\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mfill_value\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    232\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    233\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mvalues\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mF:\\Anaconda3\\lib\\site-packages\\pandas\\core\\algorithms.py\u001B[0m in \u001B[0;36mtake_nd\u001B[1;34m(arr, indexer, axis, out, fill_value, allow_fill)\u001B[0m\n\u001B[0;32m   1655\u001B[0m             \u001B[0mout\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mempty\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mout_shape\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mdtype\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0morder\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m\"F\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1656\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1657\u001B[1;33m             \u001B[0mout\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mempty\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mout_shape\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mdtype\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1658\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1659\u001B[0m     func = _get_take_nd_function(\n",
      "\u001B[1;31mMemoryError\u001B[0m: Unable to allocate 90.1 GiB for an array with shape (963, 12558485) and data type object"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "final=pd.concat([t,T.iloc[:,:-2]],axis=1)\n",
    "final.to_csv(\"final_2.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.read_csv(\"industry.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[\"code\"]=df1[\"company_code\"].apply(lambda x:re.findall(r\"(.+)_.+_.+\",x)[0]+\".\"+re.findall(r\".+_(.+)_.+\",x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-1dac48b0",
   "language": "python",
   "display_name": "PyCharm (shiyan)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}